import os
import re
import torch
import gc
import json
import soundfile as sf
import asyncio
import numpy as np
from pathlib import Path
from PySide6.QtWidgets import (
    QWidget, QVBoxLayout, QHBoxLayout, QLabel, QLineEdit, QPushButton,
    QFileDialog, QComboBox, QTextEdit, QMessageBox, QProgressBar, QCheckBox, QSplitter, QGroupBox, QTableWidget, QTableWidgetItem
)
from PySide6.QtCore import Qt, QThread, Signal, Slot
from settings import load_settings, save_settings
import srt
import subprocess
from transformers import pipeline, WhisperForConditionalGeneration, WhisperProcessor, WhisperTimeStampLogitsProcessor
from huggingface_hub import snapshot_download
from providers.openai_provider import OpenAIProvider
from providers.deepseek_provider import DeepSeekTranslator

class TranslateWorker(QThread):
    progress = Signal(int)
    log = Signal(str)
    done = Signal(str, str, str)

    def __init__(self, video_path, output_dir, engine, lang_from, lang_to, use_wuxia_style, cfg, parent=None):
        super().__init__(parent)
        self.audio_start_time = 0
        self.video_path = video_path
        self.output_dir = output_dir
        self.engine = engine
        self.lang_from = lang_from
        self.lang_to = lang_to
        self.use_wuxia_style = use_wuxia_style
        self.cfg = cfg
        self.cancel_flag = False
        self.whisper_model = None
        self.whisper_processor = None

    def extract_audio(self, video_path, output_audio="temp_audio.wav"):
        try:
            ffmpeg_path = self.cfg.get("ffmpeg_path", "ffmpeg")
            
            # Reset th·ªùi gian b·∫Øt ƒë·∫ßu
            self.audio_start_time = 0
            
            # Tr√≠ch xu·∫•t √¢m thanh v·ªõi tham s·ªë ch√≠nh x√°c h∆°n
            cmd = [
                ffmpeg_path, "-i", video_path,
                "-vn", "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2",
                "-ss", "0", "-async", "1", output_audio, "-y"
            ]
            subprocess.run(cmd, check=True)
            return output_audio
        except Exception as e:
            raise Exception(f"L·ªói tr√≠ch xu·∫•t √¢m thanh: {str(e)}")
    
    def detect_audio_start(self, audio_path):
        """Ph√°t hi·ªán th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c√≥ √¢m thanh v·ªõi x·ª≠ l√Ω l·ªói chi ti·∫øt"""
        try:
            cmd = [
                self.cfg.get("ffmpeg_path", "ffmpeg"),
                "-i", audio_path,
                "-af", "silencedetect=noise=-30dB:d=0.5",
                "-f", "null", "-"
            ]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore'
            )
            
            if result.stderr:
                for line in result.stderr.splitlines():
                    if "silence_end" in line:
                        # X·ª≠ l√Ω d√≤ng c√≥ d·∫°ng: "[silencedetect @ 0x7f] silence_end: 31.90517 | silence_duration: 2.23123"
                        parts = line.split("silence_end: ")
                        if len(parts) > 1:
                            time_part = parts[1].split()[0]  # L·∫•y ph·∫ßn tr∆∞·ªõc k√Ω t·ª± '|' ho·∫∑c kho·∫£ng tr·∫Øng
                            try:
                                return float(time_part)
                            except ValueError:
                                self.log.emit(f"C·∫£nh b√°o: Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi th·ªùi gian t·ª´ '{time_part}'")
                                continue
            
            self.log.emit("Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c th·ªùi ƒëi·ªÉm √¢m thanh b·∫Øt ƒë·∫ßu, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh 0s")
            return 0.0
            
        except Exception as e:
            self.log.emit(f"C·∫£nh b√°o: L·ªói khi ph√°t hi·ªán √¢m thanh - {str(e)}")
            return 0.0

    def load_whisper_model(self):
        model_name = "openai/whisper-large-v3"
        model_dir = Path("./models/large-v3")
        model_dir.mkdir(parents=True, exist_ok=True)
        try:
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                model_name,
                cache_dir=model_dir,
                local_files_only=True
            )
            self.whisper_processor = WhisperProcessor.from_pretrained(
                model_name,
                cache_dir=model_dir,
                local_files_only=True
            )
        except Exception:
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                model_name,
                cache_dir=model_dir
            )
            self.whisper_processor = WhisperProcessor.from_pretrained(
                model_name,
                cache_dir=model_dir
            )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.whisper_model.to(device)
        return self.whisper_model, self.whisper_processor

    def transcribe_audio(self):
        import os, re, gc, torch
        from transformers import pipeline, WhisperTimeStampLogitsProcessor
        from huggingface_hub import snapshot_download

        def split_segments_by_rules(segments, max_duration=6, max_chars=80, min_duration=0.6):
            new_segments = []
            seen_texts = set()

            for seg in segments:
                text = seg["text"].strip()
                start, end = seg["start"], seg["end"]

                if text in seen_texts:
                    continue
                seen_texts.add(text)

                # ƒê·∫∑c bi·ªát x·ª≠ l√Ω cho ti·∫øng Trung: t√°ch theo d·∫•u c√¢u ti·∫øng Trung
                if any(char in text for char in ['„ÄÇ', 'ÔºÅ', 'Ôºü', 'Ôºå']):
                    sentences = re.split(r'([„ÄÇÔºÅÔºü])', text)
                    sentences = [s.strip() for s in sentences if s.strip()]
                    # K·∫øt h·ª£p d·∫•u c√¢u v·ªõi c√¢u tr∆∞·ªõc ƒë√≥
                    sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) and len(sentences[i+1]) == 1 else '') 
                                for i in range(0, len(sentences), 2)]
                else:
                    sentences = [text]

                total_time = end - start
                if total_time <= 0:
                    continue

                time_per_char = total_time / max(1, len(text))
                t0 = start

                for sent in sentences:
                    if not sent:
                        continue
                        
                    # T√≠nh th·ªùi gian d·ª±a tr√™n ƒë·ªô d√†i c√¢u
                    seg_dur = min(len(sent) * time_per_char, max_duration)
                    seg_dur = max(seg_dur, min_duration)
                    
                    # N·∫øu c√¢u qu√° d√†i, chia nh·ªè
                    if len(sent) > max_chars:
                        parts = [sent[i:i+max_chars] for i in range(0, len(sent), max_chars)]
                        part_dur = seg_dur / len(parts)
                        for p in parts:
                            if p.strip():
                                new_segments.append({
                                    "start": t0,
                                    "end": t0 + part_dur,
                                    "text": p.strip()
                                })
                                t0 += part_dur
                    else:
                        new_segments.append({
                            "start": t0,
                            "end": t0 + seg_dur,
                            "text": sent.strip()
                        })
                        t0 += seg_dur

            return new_segments

        try:
            self.log.emit("ƒêang x·ª≠ l√Ω √¢m thanh...")
            audio_path = self.extract_audio(self.video_path)
            self.audio_start_time = 0
            self.log.emit(f"Ph√°t hi·ªán √¢m thanh b·∫Øt ƒë·∫ßu t·∫°i: {self.audio_start_time:.2f}s")

            if self.lang_from == "zh" or self.lang_to == "zh":
                self.log.emit("S·ª≠ d·ª•ng BELLE-Whisper ƒë·ªÉ phi√™n √¢m ti·∫øng Trung...")
                torch.cuda.empty_cache()
                model_path = "./models/BELLE-2/Belle-whisper-large-v3-zh"
                repo_id = "BELLE-2/Belle-whisper-large-v3-zh"
                required_files = ["config.json", "model.safetensors", "tokenizer.json", "preprocessor_config.json"]
                all_files_present = all(os.path.exists(os.path.join(model_path, f)) for f in required_files)
                if not os.path.exists(model_path) or not all_files_present:
                    snapshot_download(
                        repo_id=repo_id,
                        cache_dir="./models/",
                        repo_type="model",
                        resume_download=True,
                        ignore_patterns=["*.md", "*.txt"]
                    )
                asr_pipeline = pipeline(
                    "automatic-speech-recognition",
                    model=model_path,
                    device=0 if torch.cuda.is_available() else -1,
                    model_kwargs={"use_safetensors": True},
                    chunk_length_s=20,
                    stride_length_s=5,
                    ignore_warning=True
                )
                result = asr_pipeline(
                    audio_path,
                    return_timestamps="segment",
                    generate_kwargs={"language": "zh", "task": "transcribe"}
                )
            else:
                self.log.emit("S·ª≠ d·ª•ng Whisper ƒë·ªÉ phi√™n √¢m...")
                if not self.whisper_model or not self.whisper_processor:
                    self.whisper_model, self.whisper_processor = self.load_whisper_model()
                asr_pipeline = pipeline(
                    "automatic-speech-recognition",
                    model=self.whisper_model,
                    tokenizer=self.whisper_processor.tokenizer,
                    feature_extractor=self.whisper_processor.feature_extractor,
                    device=0 if torch.cuda.is_available() else -1,
                    chunk_length_s=20,
                    stride_length_s=5,
                    ignore_warning=True
                )
                result = asr_pipeline(
                    audio_path,
                    return_timestamps="segment",
                    generate_kwargs={
                        "language": self.lang_from,
                        "task": "transcribe",
                        "do_sample": False,
                        "num_beams": 1,
                        "logits_processor": [WhisperTimeStampLogitsProcessor(
                            generate_config=self.whisper_model.generation_config,
                            processor=self.whisper_processor,
                            begin_index=self.whisper_processor.tokenizer.sot_ids[0]
                        )]
                    }
                )

            segments = []
            for seg in result.get("chunks", []):
                start, end = seg.get("timestamp", (None, None))
                text = seg.get("text", "").strip()
                if start is None or end is None or not text:
                    self.log.emit(f"B·ªè qua ƒëo·∫°n kh√¥ng h·ª£p l·ªá: {text[:50]}...")
                    continue
                # ƒêi·ªÅu ch·ªânh th·ªùi gian theo audio_start_time
                adjusted_start = max(0, float(start) + self.audio_start_time)
                adjusted_end = max(adjusted_start + 0.1, float(end) + self.audio_start_time)
                segments.append({
                    "start": adjusted_start,
                    "end": adjusted_end, 
                    "text": text
                })

            # Log raw segments
            self.log.emit(f"T·ªïng s·ªë ƒëo·∫°n th√¥ t·ª´ BELLE-Whisper: {len(segments)}")
            for i, seg in enumerate(segments):
                self.log.emit(f"ƒêo·∫°n th√¥ {i+1}: {seg['start']} --> {seg['end']}: {seg['text'][:50]}...")

            segments = split_segments_by_rules(
                segments,
                max_duration=6.0,
                max_chars=80,
                min_duration=0.6
            )

            try:
                os.remove(audio_path)
            except:
                pass
            try:
                del asr_pipeline
            except:
                pass
            torch.cuda.empty_cache()
            try:
                torch.cuda.ipc_collect()
            except:
                pass
            gc.collect()

            if not segments:
                raise Exception("Kh√¥ng t√¨m th·∫•y ƒëo·∫°n vƒÉn b·∫£n n√†o trong √¢m thanh")

            return segments
        except Exception as e:
            self.log.emit(f"L·ªói khi phi√™n √¢m: {str(e)}")
    
    def create_subtitles(self, segments):
        """T·∫°o file SRT b·∫Øt ƒë·∫ßu t·ª´ 00:00:00"""
        subs = []
        for i, seg in enumerate(segments):
            # B·∫Øt ƒë·∫ßu lu√¥n t·ª´ 0 v√† gi·ªØ nguy√™n kho·∫£ng c√°ch gi·ªØa c√°c ƒëo·∫°n
            start_time = max(0, seg["start"])
            end_time = max(start_time + 0.1, seg["end"])
            
            subs.append(srt.Subtitle(
                index=i+1,
                start=srt.timedelta(seconds=start_time),
                end=srt.timedelta(seconds=end_time),
                content=seg["text"].strip()
            ))
        
        # S·∫Øp x·∫øp l·∫°i c√°c ƒëo·∫°n ph·ª• ƒë·ªÅ theo th·ªùi gian
        subs.sort(key=lambda x: x.start.total_seconds())
        
        # ƒê√°nh l·∫°i s·ªë th·ª© t·ª± t·ª´ 1
        for i, sub in enumerate(subs):
            sub.index = i + 1
            
        return subs

    def _format_timestamp(self, seconds):
        """Chuy·ªÉn seconds sang ƒë·ªãnh d·∫°ng SRT (HH:MM:SS,MSMS)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{seconds:06.3f}".replace('.', ',')

    async def translate_subtitles(self, subs):
        """D·ªãch ph·ª• ƒë·ªÅ s·ª≠ d·ª•ng engine ƒë√£ ch·ªçn v·ªõi Semaphore ƒë·ªÉ gi·ªõi h·∫°n request ƒë·ªìng th·ªùi"""
        try:
            subtitle_texts = [sub.content for sub in subs]
            translated_lines = []
            initial_semaphore_limit = 10
            sem = asyncio.Semaphore(initial_semaphore_limit)
            retry_delay = 5

            if self.cancel_flag:
                raise Exception("Translation canceled by user")

            provider = OpenAIProvider() if self.engine == "OpenAI" else DeepSeekTranslator(self.cfg["deepseek_api_key"])

            async def translate_with_semaphore(index, text):
                nonlocal sem
                max_retries = 3
                retries = 0

                while retries < max_retries:
                    async with sem:
                        try:
                            if len(text) > 1000:
                                text = text[:1000] + "... [TRUNCATED]"
                                self.log.emit(f"C·∫Øt ng·∫Øn d√≤ng {index+1} do qu√° d√†i: {text[:50]}...")
                            
                            self.log.emit(f"ƒêang d·ªãch d√≤ng {index+1}/{len(subtitle_texts)}: {text[:50]}...")
                            
                            if self.engine == "OpenAI":
                                result = await provider.translate(
                                    text,
                                    self.lang_from,
                                    self.lang_to,
                                    self.use_wuxia_style
                                )
                            else:
                                result = await provider.translate(
                                    text,
                                    self.lang_to,
                                    self.use_wuxia_style
                                )
                            self.log.emit(f"K·∫øt qu·∫£ d√≤ng {index+1}: {result[:50]}...")
                            return index, result.strip()

                        except Exception as e:
                            if "rate limit" in str(e).lower() or "429" in str(e):
                                self.log.emit(f"L·ªói rate limit t·∫°i d√≤ng {index+1}: {str(e)}")
                                if sem._value == initial_semaphore_limit:
                                    self.log.emit(f"Gi·∫£m semaphore t·ª´ {initial_semaphore_limit} xu·ªëng 5")
                                    sem = asyncio.Semaphore(5)
                                retries += 1
                                if retries < max_retries:
                                    self.log.emit(f"Th·ª≠ l·∫°i d√≤ng {index+1} sau {retry_delay} gi√¢y...")
                                    await asyncio.sleep(retry_delay)
                                    retry_delay *= 2
                                continue
                            else:
                                self.log.emit(f"L·ªói d·ªãch d√≤ng {index+1}: {str(e)}")
                                return index, f"[ERROR: {text}]" if self.engine == "OpenAI" else text

                self.log.emit(f"B·ªè qua d√≤ng {index+1} sau {max_retries} l·∫ßn th·ª≠")
                return index, f"[ERROR: {text}]" if self.engine == "OpenAI" else text

            tasks = [translate_with_semaphore(i, text) for i, text in enumerate(subtitle_texts)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            results = sorted(results, key=lambda x: x[0])
            translated_lines = [result[1] for result in results]

            for i in range(len(subtitle_texts)):
                if self.cancel_flag:
                    raise Exception("Translation canceled by user")
                self.progress.emit(50 + int((i + 1) / len(subtitle_texts) * 50))

            translated_subs = []
            for i, sub in enumerate(subs):
                translated_subs.append(srt.Subtitle(
                    index=sub.index,
                    start=sub.start,
                    end=sub.end,
                    content=translated_lines[i]
                ))

            return translated_subs

        except Exception as e:
            self.log.emit(f"L·ªói khi d·ªãch ph·ª• ƒë·ªÅ: {str(e)}")
            raise

    def run(self):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            self.log.emit("B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ªãch ph·ª• ƒë·ªÅ")
            self.audio_start_time = 0
            self.progress.emit(5)
            
            # 1. Tr√≠ch xu·∫•t √¢m thanh v√† ph√°t hi·ªán th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu
            audio_path = self.extract_audio(self.video_path)
            self.progress.emit(15)
            
            # 2. Phi√™n √¢m √¢m thanh
            segments = self.transcribe_audio()
            if not segments:
                raise Exception("Kh√¥ng th·ªÉ phi√™n √¢m - kh√¥ng c√≥ ƒëo·∫°n vƒÉn b·∫£n n√†o ƒë∆∞·ª£c t·∫°o")
            self.progress.emit(40)
            
            # 3. T·∫°o ph·ª• ƒë·ªÅ
            subtitles = self.create_subtitles(segments)
            self.progress.emit(50)
            
            # 4. D·ªãch ph·ª• ƒë·ªÅ
            translated_subtitles = loop.run_until_complete(
                self.translate_subtitles(subtitles)
            )
            self.progress.emit(80)
            
            # 5. L∆∞u file
            video_name = Path(self.video_path).stem
            source_srt = os.path.join(self.output_dir, f"{video_name}_{self.lang_from}.srt")
            target_srt = os.path.join(self.output_dir, f"{video_name}_{self.lang_to}.srt")
            
            with open(source_srt, "w", encoding="utf-8") as f:
                f.write(srt.compose(subtitles))
            
            with open(target_srt, "w", encoding="utf-8") as f:
                f.write(srt.compose(translated_subtitles))
            
            self.progress.emit(95)
            
            # D·ªçn d·∫πp
            if os.path.exists(audio_path):
                os.remove(audio_path)
            torch.cuda.empty_cache()
            gc.collect()
            
            self.progress.emit(100)
            self.done.emit("success", "D·ªãch ph·ª• ƒë·ªÅ ho√†n t·∫•t", target_srt)
        
        except Exception as e:
            self.log.emit(f"L·ªói: {str(e)}")
            self.done.emit("error", str(e), "")
        
        finally:
            loop.close()

class TranslateTab(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.cfg = load_settings()
        self.source_srt_path = None
        self.target_srt_path = None
        self.init_ui()
        self.worker = None

    def init_ui(self):
        # Layout ch√≠nh
        main_layout = QVBoxLayout(self)

        # --- Ph·∫ßn ƒëi·ªÅu khi·ªÉn tr√™n c√πng ---
        control_widget = QWidget()
        control_layout = QVBoxLayout(control_widget)
        
        # Video input
        video_layout = QHBoxLayout()
        video_layout.addWidget(QLabel("Video:"))
        self.video_input = QLineEdit()
        video_layout.addWidget(self.video_input)
        btn_video = QPushButton("Ch·ªçn...")
        btn_video.clicked.connect(self.select_video)
        video_layout.addWidget(btn_video)
        control_layout.addLayout(video_layout)
        
        # Output directory
        output_layout = QHBoxLayout()
        output_layout.addWidget(QLabel("Th∆∞ m·ª•c ƒë·∫ßu ra:"))
        self.output_dir_input = QLineEdit(self.cfg.get("out_dir", str(Path(__file__).parent / "subtitles")))
        output_layout.addWidget(self.output_dir_input)
        btn_output = QPushButton("Ch·ªçn...")
        btn_output.clicked.connect(self.select_output_dir)
        output_layout.addWidget(btn_output)
        control_layout.addLayout(output_layout)
        
        # Translation settings
        settings_layout = QHBoxLayout()
        settings_layout.addWidget(QLabel("Engine:"))
        self.engine_combo = QComboBox()
        self.engine_combo.addItems(["OpenAI", "DeepSeek"])
        settings_layout.addWidget(self.engine_combo)
        
        settings_layout.addWidget(QLabel("T·ª´:"))
        self.lang_from_combo = QComboBox()
        self.lang_from_combo.addItems(["en", "vi", "zh", "ja", "ko"])
        settings_layout.addWidget(self.lang_from_combo)
        
        settings_layout.addWidget(QLabel("Sang:"))
        self.lang_to_combo = QComboBox()
        self.lang_to_combo.addItems(["en", "vi", "zh", "ja", "ko"])
        settings_layout.addWidget(self.lang_to_combo)
        
        self.wuxia_check = QCheckBox("Phong c√°ch ki·∫øm hi·ªáp (Trung-Vi·ªát)")
        self.wuxia_check.setEnabled(False)
        settings_layout.addWidget(self.wuxia_check)
        control_layout.addLayout(settings_layout)
        
        # Buttons
        btn_layout = QHBoxLayout()
        self.start_btn = QPushButton("B·∫Øt ƒë·∫ßu d·ªãch")
        self.start_btn.clicked.connect(self.start_translation)
        btn_layout.addWidget(self.start_btn)
        
        self.cancel_btn = QPushButton("H·ªßy b·ªè")
        self.cancel_btn.clicked.connect(self.cancel_translation)
        self.cancel_btn.setEnabled(False)
        btn_layout.addWidget(self.cancel_btn)
        control_layout.addLayout(btn_layout)
        
        # Progress bar
        self.progress_bar = QProgressBar()
        self.progress_bar.setRange(0, 100)
        control_layout.addWidget(self.progress_bar)
        
        main_layout.addWidget(control_widget)

         # --- Ph·∫ßn hi·ªÉn th·ªã SRT ---
        self.srt_splitter = QSplitter(Qt.Horizontal)
        
        # Khung SRT ngu·ªìn (cho ph√©p ch·ªânh s·ª≠a)
        source_group = QGroupBox("Ph·ª• ƒë·ªÅ ngu·ªìn")
        source_layout = QVBoxLayout(source_group)
        self.source_table = QTableWidget()
        self.source_table.setColumnCount(3)
        self.source_table.setHorizontalHeaderLabels(["Start", "End", "Text"])
        self.source_table.horizontalHeader().setStretchLastSection(True)
        self.source_table.verticalHeader().setVisible(False)
        self.source_table.setEditTriggers(QTableWidget.DoubleClicked | QTableWidget.EditKeyPressed)
        source_layout.addWidget(self.source_table)
        
        # N√∫t l∆∞u v√† th√™m n√∫t ch·ªânh s·ª≠a th·ªùi gian
        btn_layout_source = QHBoxLayout()
        self.save_source_btn = QPushButton("L∆∞u ph·ª• ƒë·ªÅ ngu·ªìn")
        self.save_source_btn.clicked.connect(self.save_source_srt)
        btn_layout_source.addWidget(self.save_source_btn)
        
        self.edit_time_btn_source = QPushButton("Ch·ªânh th·ªùi gian")
        self.edit_time_btn_source.clicked.connect(lambda: self.show_time_editor(self.source_table))
        btn_layout_source.addWidget(self.edit_time_btn_source)
        source_layout.addLayout(btn_layout_source)
        
        # Khung SRT ƒë√≠ch (cho ph√©p ch·ªânh s·ª≠a)
        target_group = QGroupBox("Ph·ª• ƒë·ªÅ ƒë√≠ch")
        target_layout = QVBoxLayout(target_group)
        self.target_table = QTableWidget()
        self.target_table.setColumnCount(3)
        self.target_table.setHorizontalHeaderLabels(["Start", "End", "Text"])
        self.target_table.horizontalHeader().setStretchLastSection(True)
        self.target_table.verticalHeader().setVisible(False)
        self.target_table.setEditTriggers(QTableWidget.DoubleClicked | QTableWidget.EditKeyPressed)
        target_layout.addWidget(self.target_table)
        
        # N√∫t l∆∞u v√† th√™m n√∫t ch·ªânh s·ª≠a th·ªùi gian
        btn_layout_target = QHBoxLayout()
        self.save_target_btn = QPushButton("L∆∞u ph·ª• ƒë·ªÅ ƒë√≠ch")
        self.save_target_btn.clicked.connect(self.save_target_srt)
        btn_layout_target.addWidget(self.save_target_btn)
        
        self.edit_time_btn_target = QPushButton("Ch·ªânh th·ªùi gian")
        self.edit_time_btn_target.clicked.connect(lambda: self.show_time_editor(self.target_table))
        btn_layout_target.addWidget(self.edit_time_btn_target)
        target_layout.addLayout(btn_layout_target)
        
        self.srt_splitter.addWidget(source_group)
        self.srt_splitter.addWidget(target_group)
        self.srt_splitter.setSizes([400, 400])
        
        main_layout.addWidget(self.srt_splitter, 1)

        # --- Ph·∫ßn log (ƒë∆°n gi·∫£n) ---
        self.log_output = QTextEdit()
        self.log_output.setReadOnly(True)
        self.log_output.setMaximumHeight(100)  # Gi·ªõi h·∫°n chi·ªÅu cao log
        main_layout.addWidget(self.log_output)

        # K·∫øt n·ªëi signal
        self.lang_from_combo.currentTextChanged.connect(self.update_wuxia_option)
        self.lang_to_combo.currentTextChanged.connect(self.update_wuxia_option)
        self.update_wuxia_option()
    
    def show_time_editor(self, table):
        """Hi·ªÉn th·ªã dialog ch·ªânh s·ª≠a th·ªùi gian cho d√≤ng ƒë∆∞·ª£c ch·ªçn"""
        selected_row = table.currentRow()
        if selected_row < 0:
            QMessageBox.warning(self, "C·∫£nh b√°o", "Vui l√≤ng ch·ªçn m·ªôt d√≤ng ƒë·ªÉ ch·ªânh s·ª≠a")
            return
        
        start_time = table.item(selected_row, 0).text()
        end_time = table.item(selected_row, 1).text()
        
        dialog = QDialog(self)
        dialog.setWindowTitle("Ch·ªânh s·ª≠a th·ªùi gian")
        layout = QVBoxLayout(dialog)
        
        # √î nh·∫≠p th·ªùi gian b·∫Øt ƒë·∫ßu
        start_layout = QHBoxLayout()
        start_layout.addWidget(QLabel("Th·ªùi gian b·∫Øt ƒë·∫ßu:"))
        start_edit = QLineEdit(start_time)
        start_layout.addWidget(start_edit)
        
        # √î nh·∫≠p th·ªùi gian k·∫øt th√∫c
        end_layout = QHBoxLayout()
        end_layout.addWidget(QLabel("Th·ªùi gian k·∫øt th√∫c:"))
        end_edit = QLineEdit(end_time)
        end_layout.addWidget(end_edit)
        
        # N√∫t x√°c nh·∫≠n
        btn_box = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)
        btn_box.accepted.connect(dialog.accept)
        btn_box.rejected.connect(dialog.reject)
        
        layout.addLayout(start_layout)
        layout.addLayout(end_layout)
        layout.addWidget(btn_box)
        
        if dialog.exec() == QDialog.Accepted:
            table.setItem(selected_row, 0, QTableWidgetItem(start_edit.text()))
            table.setItem(selected_row, 1, QTableWidgetItem(end_edit.text()))
    
    def load_srt_to_table(self, table, file_path):
        """T·∫£i n·ªôi dung SRT v√†o QTableWidget v·ªõi ki·ªÉm tra l·ªói"""
        try:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File kh√¥ng t·ªìn t·∫°i: {file_path}")
                
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            table.setRowCount(0)  # X√≥a d·ªØ li·ªáu c≈©
            
            # S·ª≠ d·ª•ng parser c·ªßa th∆∞ vi·ªán srt thay v√¨ regex
            subs = list(srt.parse(content))
            table.setRowCount(len(subs))
            
            for row, sub in enumerate(subs):
                table.setItem(row, 0, QTableWidgetItem(str(sub.start)))
                table.setItem(row, 1, QTableWidgetItem(str(sub.end)))
                table.setItem(row, 2, QTableWidgetItem(sub.content))
                
        except Exception as e:
            QMessageBox.warning(self, "L·ªói", f"Kh√¥ng th·ªÉ ƒë·ªçc SRT: {str(e)}")
    
    def save_table_to_srt(self, table, file_path):
        with open(file_path, "w", encoding="utf-8") as f:
            for row in range(table.rowCount()):
                start = table.item(row, 0).text() if table.item(row, 0) else ""
                end = table.item(row, 1).text() if table.item(row, 1) else ""
                text = table.item(row, 2).text() if table.item(row, 2) else ""
                f.write(f"{row+1}\n{start} --> {end}\n{text}\n\n")
        print(f"üíæ Saved SRT: {file_path}")

    def toggle_log_visibility(self, visible):
        """·∫®n/hi·ªán ph·∫ßn log"""
        if visible:
            self.log_output.setMaximumHeight(100)
        else:
            self.log_output.setMaximumHeight(0)
    
    def load_srt_files(self, source_path, target_path):
        """T·∫£i n·ªôi dung SRT v√†o c√°c khung xem"""
        try:
            with open(source_path, 'r', encoding='utf-8') as f:
                self.source_srt_edit.setPlainText(f.read())
            self.source_srt_path = source_path
            
            if target_path and os.path.exists(target_path):
                with open(target_path, 'r', encoding='utf-8') as f:
                    self.target_srt_edit.setPlainText(f.read())
                self.target_srt_path = target_path
        except Exception as e:
            QMessageBox.warning(self, "L·ªói", f"Kh√¥ng th·ªÉ t·∫£i file SRT: {str(e)}")

    def save_source_srt(self):
        """L∆∞u ph·ª• ƒë·ªÅ ngu·ªìn"""
        if not hasattr(self, 'source_srt_path') or not self.source_srt_path:
            path, _ = QFileDialog.getSaveFileName(
                self, "L∆∞u ph·ª• ƒë·ªÅ ngu·ªìn", "",
                "Subtitle Files (*.srt)"
            )
            if not path:
                return
            self.source_srt_path = path
        
        self.save_table_to_srt(self.source_table, self.source_srt_path)

    def save_target_srt(self):
        """L∆∞u ph·ª• ƒë·ªÅ ƒë√≠ch"""
        if not hasattr(self, 'target_srt_path') or not self.target_srt_path:
            path, _ = QFileDialog.getSaveFileName(
                self, "L∆∞u ph·ª• ƒë·ªÅ ƒë√≠ch", "",
                "Subtitle Files (*.srt)"
            )
            if not path:
                return
            self.target_srt_path = path
        
        self.save_table_to_srt(self.target_table, self.target_srt_path)

    # S·ª≠a h√†m on_translation_done ƒë·ªÉ t·∫£i SRT khi ho√†n th√†nh
    def on_translation_done(self, status, message, output_path):
        self.start_btn.setEnabled(True)
        self.cancel_btn.setEnabled(False)
        
        if status == "success":
            video_name = Path(self.video_input.text()).stem
            output_dir = self.output_dir_input.text()
            
            # ƒê∆∞·ªùng d·∫´n file SRT ngu·ªìn v√† ƒë√≠ch
            source_srt = Path(output_dir) / f"{video_name}_{self.lang_from_combo.currentText()}.srt"
            target_srt = Path(output_dir) / f"{video_name}_{self.lang_to_combo.currentText()}.srt"
            
            # T·∫£i n·ªôi dung v√†o khung xem
            self.load_srt_files(str(source_srt), str(target_srt))
            
            QMessageBox.information(
                self, 
                "Th√†nh c√¥ng", 
                f"{message}\nFile ƒë√£ l∆∞u t·∫°i: {output_path}"
            )
        else:
            QMessageBox.critical(self, "L·ªói", message)
    
    def add_find_replace_toolbars(self):
        """Th√™m thanh c√¥ng c·ª• Find & Replace cho c·∫£ 2 khung SRT"""
        # Toolbar cho khung ngu·ªìn
        source_toolbar = QHBoxLayout()
        source_toolbar.addWidget(QLabel("T√¨m ki·∫øm:"))
        self.source_find_edit = QLineEdit()
        self.source_find_edit.setPlaceholderText("Nh·∫≠p t·ª´ c·∫ßn t√¨m...")
        source_toolbar.addWidget(self.source_find_edit)
        
        self.source_replace_edit = QLineEdit()
        self.source_replace_edit.setPlaceholderText("Nh·∫≠p t·ª´ thay th·∫ø...")
        source_toolbar.addWidget(self.source_replace_edit)
        
        self.source_find_btn = QPushButton("T√¨m")
        self.source_find_btn.clicked.connect(lambda: self.find_text(self.source_srt_edit, self.source_find_edit))
        source_toolbar.addWidget(self.source_find_btn)
        
        self.source_replace_btn = QPushButton("Thay th·∫ø")
        self.source_replace_btn.clicked.connect(lambda: self.replace_text(self.source_srt_edit, self.source_find_edit, self.source_replace_edit))
        source_toolbar.addWidget(self.source_replace_btn)
        
        self.source_replace_all_btn = QPushButton("Thay t·∫•t c·∫£")
        self.source_replace_all_btn.clicked.connect(lambda: self.replace_all_text(self.source_srt_edit, self.source_find_edit, self.source_replace_edit))
        source_toolbar.addWidget(self.source_replace_all_btn)
        
        # Th√™m v√†o layout khung ngu·ªìn
        self.source_srt_widget.layout().insertLayout(1, source_toolbar)
        
        # Toolbar t∆∞∆°ng t·ª± cho khung ƒë√≠ch
        target_toolbar = QHBoxLayout()
        target_toolbar.addWidget(QLabel("T√¨m ki·∫øm:"))
        self.target_find_edit = QLineEdit()
        self.target_find_edit.setPlaceholderText("Nh·∫≠p t·ª´ c·∫ßn t√¨m...")
        target_toolbar.addWidget(self.target_find_edit)
        
        self.target_replace_edit = QLineEdit()
        self.target_replace_edit.setPlaceholderText("Nh·∫≠p t·ª´ thay th·∫ø...")
        target_toolbar.addWidget(self.target_replace_edit)
        
        self.target_find_btn = QPushButton("T√¨m")
        self.target_find_btn.clicked.connect(lambda: self.find_text(self.target_srt_edit, self.target_find_edit))
        target_toolbar.addWidget(self.target_find_btn)
        
        self.target_replace_btn = QPushButton("Thay th·∫ø")
        self.target_replace_btn.clicked.connect(lambda: self.replace_text(self.target_srt_edit, self.target_find_edit, self.target_replace_edit))
        target_toolbar.addWidget(self.target_replace_btn)
        
        self.target_replace_all_btn = QPushButton("Thay t·∫•t c·∫£")
        self.target_replace_all_btn.clicked.connect(lambda: self.replace_all_text(self.target_srt_edit, self.target_find_edit, self.target_replace_edit))
        target_toolbar.addWidget(self.target_replace_all_btn)
        
        # Th√™m v√†o layout khung ƒë√≠ch
        self.target_srt_widget.layout().insertLayout(1, target_toolbar)
    
    def find_text(self, text_edit, find_edit):
        """T√¨m ki·∫øm text trong QTextEdit v·ªõi highlight"""
        text_to_find = find_edit.text()
        if not text_to_find:
            return
            
        # X√≥a highlight tr∆∞·ªõc ƒë√≥
        self.clear_highlight(text_edit)
        
        document = text_edit.document()
        cursor = text_edit.textCursor()
        format = QTextCharFormat()
        format.setBackground(QColor("yellow"))
        
        # T√¨m t·∫•t c·∫£ c√°c k·∫øt qu·∫£ v√† highlight
        count = 0
        cursor = document.find(text_to_find, 0)
        while not cursor.isNull():
            cursor.mergeCharFormat(format)
            count += 1
            cursor = document.find(text_to_find, cursor)
            
        # Di chuy·ªÉn ƒë·∫øn k·∫øt qu·∫£ ƒë·∫ßu ti√™n
        cursor = document.find(text_to_find, 0)
        if not cursor.isNull():
            text_edit.setTextCursor(cursor)
            text_edit.setFocus()
        
        QMessageBox.information(self, "Th√¥ng b√°o", f"T√¨m th·∫•y {count} k·∫øt qu·∫£")
    
    def clear_highlight(self, text_edit):
        """X√≥a t·∫•t c·∫£ highlight"""
        cursor = text_edit.textCursor()
        cursor.movePosition(QTextCursor.Start)
        
        format = QTextCharFormat()
        format.setBackground(QColor("white"))
        
        cursor.select(QTextCursor.Document)
        cursor.mergeCharFormat(format)
        text_edit.setFocus()
    
    def replace_text(self, text_edit, find_edit, replace_edit):
        """Thay th·∫ø text hi·ªán t·∫°i ƒë∆∞·ª£c t√¨m th·∫•y"""
        cursor = text_edit.textCursor()
        if cursor.hasSelection():
            cursor.insertText(replace_edit.text())
        self.find_text(text_edit, find_edit)  # T√¨m ti·∫øp
    
    def replace_all_text(self, text_edit, find_edit, replace_edit):
        """Thay th·∫ø t·∫•t c·∫£ text ph√π h·ª£p"""
        text_to_find = find_edit.text()
        if not text_to_find:
            return
            
        # L·∫•y to√†n b·ªô text
        text = text_edit.toPlainText()
        replaced_text = text.replace(text_to_find, replace_edit.text())
        
        # C·∫≠p nh·∫≠t text m·ªõi
        text_edit.setPlainText(replaced_text)
        
        QMessageBox.information(self, "Th√¥ng b√°o", f"ƒê√£ thay th·∫ø {text.count(text_to_find)} l·∫ßn xu·∫•t hi·ªán")

    def update_wuxia_option(self):
        """C·∫≠p nh·∫≠t tr·∫°ng th√°i checkbox wuxia"""
        from_zh = self.lang_from_combo.currentText() == "zh"
        to_vi = self.lang_to_combo.currentText() == "vi"
        self.wuxia_check.setEnabled(from_zh and to_vi)
        if not (from_zh and to_vi):
            self.wuxia_check.setChecked(False)

    def select_video(self):
        path, _ = QFileDialog.getOpenFileName(
            self, "Ch·ªçn video", "", 
            "Video Files (*.mp4 *.mkv *.avi *.mov)"
        )
        if path:
            self.video_input.setText(path)

    def select_output_dir(self):
        path = QFileDialog.getExistingDirectory(
            self, 
            "Ch·ªçn th∆∞ m·ª•c ƒë·∫ßu ra", 
            self.output_dir_input.text()
        )
        if path:
            self.output_dir_input.setText(path)

    def start_translation(self):
        # Validate inputs
        video_path = self.video_input.text().strip()
        if not video_path or not Path(video_path).exists():
            QMessageBox.warning(self, "L·ªói", "Vui l√≤ng ch·ªçn file video h·ª£p l·ªá")
            return
            
        output_dir = self.output_dir_input.text().strip()
        if not output_dir:
            QMessageBox.warning(self, "L·ªói", "Vui l√≤ng ch·ªçn th∆∞ m·ª•c ƒë·∫ßu ra")
            return
            
        # Ki·ªÉm tra v√† t·∫°o th∆∞ m·ª•c ƒë·∫ßu ra
        try:
            Path(output_dir).mkdir(parents=True, exist_ok=True)
        except Exception as e:
            QMessageBox.critical(self, "L·ªói", f"Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c: {str(e)}")
            return
            
        # Ki·ªÉm tra API keys
        engine = self.engine_combo.currentText()
        if engine == "OpenAI" and not self.cfg.get("openai_api_key"):
            QMessageBox.warning(self, "L·ªói", "Thi·∫øu OpenAI API key")
            return
        elif engine == "DeepSeek" and not self.cfg.get("deepseek_api_key"):
            QMessageBox.warning(self, "L·ªói", "Thi·∫øu DeepSeek API key")
            return
            
        # L∆∞u c√†i ƒë·∫∑t
        self.cfg.update({
            "out_dir": output_dir,
            "use_wuxia_style": self.wuxia_check.isChecked()
        })
        save_settings(self.cfg)
        
        # Chu·∫©n b·ªã UI
        self.log_output.clear()
        self.progress_bar.setValue(0)
        self.start_btn.setEnabled(False)
        self.cancel_btn.setEnabled(True)
        
        # Kh·ªüi t·∫°o worker
        self.worker = TranslateWorker(
            video_path=video_path,
            output_dir=output_dir,
            engine=engine,
            lang_from=self.lang_from_combo.currentText(),
            lang_to=self.lang_to_combo.currentText(),
            use_wuxia_style=self.wuxia_check.isChecked(),
            cfg=self.cfg
        )
        self.worker.progress.connect(self.progress_bar.setValue)
        self.worker.log.connect(self.log_output.append)
        self.worker.done.connect(self.on_translation_done)
        self.worker.start()

    def cancel_translation(self):
        if self.worker:
            self.worker.cancel()
            self.log_output.append("ƒêang h·ªßy qu√° tr√¨nh...")
            self.cancel_btn.setEnabled(False)

    def on_translation_done(self, status, message, output_path):
        self.start_btn.setEnabled(True)
        self.cancel_btn.setEnabled(False)
        
        if status == "success":
            QMessageBox.information(
                self, 
                "Th√†nh c√¥ng", 
                f"{message}\nFile ƒë√£ l∆∞u t·∫°i: {output_path}"
            )
        else:
            QMessageBox.critical(self, "L·ªói", message)